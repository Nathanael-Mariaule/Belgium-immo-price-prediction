{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we load the datas and after a simple preprocessing, we try various regression model to see which one achieve the best performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNetCV, LassoCV, RidgeCV, Lasso, SGDRegressor\n",
    "from data_cleaner import eliza_cleaning, eliza_fillna\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, Normalizer, StandardScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import explained_variance_score as evs # evaluation metric\n",
    "from sklearn.metrics import r2_score as r2 # evaluation metric\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datas = pd.read_csv('https://raw.githubusercontent.com/JulienAlardot/challenge-collecting-data/main/Data/database.csv')\n",
    "datas = eliza_cleaning(raw_datas)\n",
    "datas = eliza_fillna(datas)\n",
    "#datas.drop(columns=['locality'], inplace=True)\n",
    "house = datas[datas['type_of_property']=='house'].copy()\n",
    "appart = datas[datas['type_of_property']=='apartment'].copy()\n",
    "datas = datas[datas.price>50000]\n",
    "y = datas.pop('price')\n",
    "X = datas\n",
    "house_y = house.pop('price')\n",
    "house_x = house\n",
    "appart_y = appart.pop('price')\n",
    "appart_x = appart\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "\n",
    "fillna = ColumnTransformer(\n",
    "        [ ('imp', KNNImputer(n_neighbors=2, weights=\"uniform\"), list(range(1,12)))],\n",
    "         remainder='passthrough')\n",
    "fillna = ColumnTransformer(\n",
    "        [('imp_col1', SimpleImputer(strategy='mean'), ['area', 'terrace_area', 'garden_area', \n",
    "                                                      'surface_of_the_land']),\n",
    "         ('imp_col2', SimpleImputer(strategy='median'), ['number_of_rooms', 'number_of_facades']),\n",
    "        ],remainder='passthrough')\n",
    "enc = ColumnTransformer(\n",
    "        [\n",
    "         ('enc', OneHotEncoder(sparse = False, drop ='first'), [-4, -3,-2,-1]),\n",
    "         #('enc2', OneHotEncoder(sparse = False, handle_unknown='ignore'), [-7])\n",
    "        ], remainder='passthrough')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(fillna, enc, Normalizer(), PolynomialFeatures(2), model )\n",
    "pipe.fit(X_train, y_train)\n",
    "preds = pipe.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "print(pipe.score(X_val, y_val))\n",
    "print(evs(y_val, preds))\n",
    "print(r2(y_val, preds))\n",
    "\n",
    "preds_train = pipe.predict(X_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, preds_train))\n",
    "print(\"RMSE_Train: %f\" % (rmse_train))\n",
    "print(pipe.score(X_train, y_train))\n",
    "print(evs(y_train, preds_train))\n",
    "print(r2(y_train, preds_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(fillna, enc, StandardScaler())\n",
    "pipe.fit(X_train)\n",
    "X_train = pipe.transform(X_train)\n",
    "X_val = pipe.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticeNet model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.510622\n",
      "0.3055113531185044\n",
      "0.3055345528609442\n",
      "RMSE_Train: 0.509004\n",
      "0.30966406006713276\n",
      "0.30966406006713276\n"
     ]
    }
   ],
   "source": [
    "alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\n",
    "alphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n",
    "e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n",
    "e_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n",
    "kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "model =  ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio)\n",
    "model.fit(X_train, np.log1p(y_train))\n",
    "preds = model.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error( np.log1p(y_val), preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "print(model.score(X_val,  np.log1p(y_val)))\n",
    "print(evs( np.log1p(y_val), preds))\n",
    "\n",
    "preds_train = model.predict(X_train)\n",
    "rmse_train = np.sqrt(mean_squared_error( np.log1p(y_train), preds_train))\n",
    "print(\"RMSE_Train: %f\" % (rmse_train))\n",
    "print(model.score(X_train,  np.log1p(y_train)))\n",
    "print(evs( np.log1p(y_train),  preds_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial RidgeRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.507959\n",
      "0.312735116688046\n",
      "0.31275222174227213\n",
      "RMSE_Train: 0.506765\n",
      "0.3157230210377473\n",
      "0.3157230210377474\n"
     ]
    }
   ],
   "source": [
    "model =  make_pipeline(PolynomialFeatures(2), Ridge(5))\n",
    "model.fit(X_train, np.log1p(y_train))\n",
    "preds = model.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error( np.log1p(y_val), preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "print(model.score(X_val,  np.log1p(y_val)))\n",
    "print(evs( np.log1p(y_val), preds))\n",
    "\n",
    "preds_train = model.predict(X_train)\n",
    "rmse_train = np.sqrt(mean_squared_error( np.log1p(y_train), preds_train))\n",
    "print(\"RMSE_Train: %f\" % (rmse_train))\n",
    "print(model.score(X_train,  np.log1p(y_train)))\n",
    "print(evs( np.log1p(y_train),  preds_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial LASSO regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.613471\n",
      "0.0025255843666625033\n",
      "0.00252933853418158\n",
      "RMSE_Train: 0.597570\n",
      "0.006878350529813004\n",
      "0.006878350529813004\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(PolynomialFeatures(2),Lasso(1))\n",
    "model.fit(X_train, np.log1p(y_train))\n",
    "preds = model.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error( np.log1p(y_val), preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "print(model.score(X_val,  np.log1p(y_val)))\n",
    "print(evs( np.log1p(y_val), preds))\n",
    "\n",
    "preds_train = model.predict(X_train)\n",
    "rmse_train = np.sqrt(mean_squared_error( np.log1p(y_train), preds_train))\n",
    "print(\"RMSE_Train: %f\" % (rmse_train))\n",
    "print(model.score(X_train,  np.log1p(y_train)))\n",
    "print(evs( np.log1p(y_train),  preds_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE for XGBoost Model : 91290.46625236742\n",
      "Validation MAE for multi-pass XGBoost Model : 89948.58695819805\n",
      "RMSE: 0.299181\n",
      "0.7627636546822527\n",
      "0.7628442797722688\n",
      "RMSE_Train: 0.265319\n",
      "0.8042234441848317\n",
      "0.8042234441926572\n",
      "RMSE: 0.299181\n",
      "0.7627636546822527\n",
      "0.7628442797722688\n",
      "RMSE_Train: 0.265319\n",
      "0.8042234441848317\n",
      "0.8042234441926572\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "my_XGB_model = XGBRegressor()\n",
    "my_XGB_model.fit(X_train, np.log(y_train), verbose=False)\n",
    "\n",
    "# make predictions\n",
    "XGB_predictions = my_XGB_model.predict(X_val)\n",
    "XGB_predictions = np.exp(XGB_predictions)\n",
    "# Print MAE for initial XGB model\n",
    "XGB_mae = mean_absolute_error(XGB_predictions, y_val)\n",
    "print(\"Validation MAE for XGBoost Model : \" + str(XGB_mae))\n",
    "      \n",
    "# Additional Passes\n",
    "my_XGB_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "my_XGB_model.fit(X_train, np.log(y_train), early_stopping_rounds=5, \n",
    "             eval_set=[(X_val, np.log(y_val))], verbose=False)\n",
    "XGB_predictions = my_XGB_model.predict(X_val)\n",
    "XGB_predictions = np.exp(XGB_predictions)\n",
    "XGB_mult_mae = mean_absolute_error(XGB_predictions, y_val)\n",
    "print(\"Validation MAE for multi-pass XGBoost Model : \" + str(XGB_mult_mae))\n",
    "\n",
    "\n",
    "\n",
    "preds = my_XGB_model.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error( np.log(y_val), preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "print(my_XGB_model.score(X_val,  np.log(y_val)))\n",
    "print(evs( np.log1p(y_val), preds))\n",
    "\n",
    "preds_train = my_XGB_model.predict(X_train)\n",
    "rmse_train = np.sqrt(mean_squared_error( np.log(y_train), preds_train))\n",
    "print(\"RMSE_Train: %f\" % (rmse_train))\n",
    "print(my_XGB_model.score(X_train,  np.log(y_train)))\n",
    "print(evs( np.log(y_train),  preds_train))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "print(my_XGB_model.score(X_val,  np.log(y_val)))\n",
    "print(evs( np.log(y_val), preds))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE for XGBoost Model : 92382.0016101867\n",
      "Validation MAE for multi-pass XGBoost Model : 89508.91307968073\n",
      "RMSE: 0.296273\n",
      "0.7673526209578967\n",
      "0.7674292101448412\n",
      "RMSE_Train: 0.242655\n",
      "0.8362428318096249\n",
      "0.8362428319105412\n"
     ]
    }
   ],
   "source": [
    "poly = PolynomialFeatures(2)\n",
    "poly.fit(X_train)\n",
    "\n",
    "my_XGB_model = XGBRegressor()\n",
    "my_XGB_model.fit(poly.transform(X_train), np.log(y_train), verbose=False)\n",
    "\n",
    "# make predictions\n",
    "XGB_predictions = my_XGB_model.predict(poly.transform(X_val))\n",
    "XGB_predictions = np.exp(XGB_predictions)\n",
    "# Print MAE for initial XGB model\n",
    "XGB_mae = mean_absolute_error(XGB_predictions, y_val)\n",
    "print(\"Validation MAE for XGBoost Model : \" + str(XGB_mae))\n",
    "      \n",
    "# Additional Passes\n",
    "my_XGB_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "my_XGB_model.fit(poly.transform(X_train), np.log(y_train), early_stopping_rounds=5, \n",
    "             eval_set=[(poly.transform(X_val), np.log(y_val))], verbose=False)\n",
    "XGB_predictions = my_XGB_model.predict(poly.transform(X_val))\n",
    "XGB_predictions = np.exp(XGB_predictions)\n",
    "XGB_mult_mae = mean_absolute_error(XGB_predictions, y_val)\n",
    "print(\"Validation MAE for multi-pass XGBoost Model : \" + str(XGB_mult_mae))\n",
    "\n",
    "\n",
    "\n",
    "preds = my_XGB_model.predict(poly.transform(X_val))\n",
    "rmse = np.sqrt(mean_squared_error( np.log1p(y_val), preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "print(my_XGB_model.score(poly.transform(X_val),  np.log1p(y_val)))\n",
    "print(evs( np.log(y_val), preds))\n",
    "\n",
    "preds_train = my_XGB_model.predict(poly.transform(X_train))\n",
    "rmse_train = np.sqrt(mean_squared_error( np.log(y_train), preds_train))\n",
    "print(\"RMSE_Train: %f\" % (rmse_train))\n",
    "print(my_XGB_model.score(poly.transform(X_train),  np.log(y_train)))\n",
    "print(evs( np.log(y_train),  preds_train))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.577182\n",
      "0.11704270548828088\n",
      "0.11764209670120385\n",
      "RMSE_Train: 0.454427\n",
      "0.4256801833173314\n",
      "0.42571124577496877\n"
     ]
    }
   ],
   "source": [
    "model = SGDRegressor(max_iter=10*12, tol=1e-3)\n",
    "model.fit(X_train, np.log1p(y_train))\n",
    "preds = model.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error( np.log1p(y_val), preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "print(model.score(X_val,  np.log1p(y_val)))\n",
    "print(evs( np.log1p(y_val), preds))\n",
    "\n",
    "preds_train = model.predict(X_train)\n",
    "rmse_train = np.sqrt(mean_squared_error( np.log1p(y_train), preds_train))\n",
    "print(\"RMSE_Train: %f\" % (rmse_train))\n",
    "print(model.score(X_train,  np.log1p(y_train)))\n",
    "print(evs( np.log1p(y_train),  preds_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial RidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = make_pipeline(PolynomialFeatures(2), RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]))\n",
    "model.fit(X_train, np.log1p(y_train))\n",
    "preds = model.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error( np.log1p(y_val), preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "print(model.score(X_val,  np.log1p(y_val)))\n",
    "print(evs( np.log1p(y_val), preds))\n",
    "\n",
    "preds_train = model.predict(X_train)\n",
    "rmse_train = np.sqrt(mean_squared_error( np.log1p(y_train), preds_train))\n",
    "print(\"RMSE_Train: %f\" % (rmse_train))\n",
    "print(model.score(X_train,  np.log1p(y_train)))\n",
    "print(evs( np.log1p(y_train),  preds_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.442299\n",
      "0.4815030602166884\n",
      "0.481563756322101\n",
      "RMSE_Train: 0.423742\n",
      "0.5006227333347059\n",
      "0.5006227333347059\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lars\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(2), Lars(n_nonzero_coefs=40))\n",
    "model.fit(X_train, np.log1p(y_train))\n",
    "preds = model.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error( np.log1p(y_val), preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "print(model.score(X_val,  np.log1p(y_val)))\n",
    "print(evs( np.log1p(y_val), preds))\n",
    "\n",
    "preds_train = model.predict(X_train)\n",
    "rmse_train = np.sqrt(mean_squared_error( np.log1p(y_train), preds_train))\n",
    "print(\"RMSE_Train: %f\" % (rmse_train))\n",
    "print(model.score(X_train,  np.log1p(y_train)))\n",
    "from sklearn.linear_model import Lars\n",
    "print(evs( np.log1p(y_train),  preds_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.427820\n",
      "0.5148938307184496\n",
      "0.514963611363113\n",
      "RMSE_Train: 0.380648\n",
      "0.5970303754128237\n",
      "0.5970303754128237\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(2),BayesianRidge(n_iter=2000))\n",
    "model.fit(X_train, np.log1p(y_train))\n",
    "preds = model.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error( np.log1p(y_val), preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "print(model.score(X_val,  np.log1p(y_val)))\n",
    "print(evs( np.log1p(y_val), preds))\n",
    "\n",
    "preds_train = model.predict(X_train)\n",
    "rmse_train = np.sqrt(mean_squared_error( np.log1p(y_train), preds_train))\n",
    "print(\"RMSE_Train: %f\" % (rmse_train))\n",
    "print(model.score(X_train,  np.log1p(y_train)))\n",
    "print(evs( np.log1p(y_train),  preds_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we achieve the best performance with XGBoost by far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
